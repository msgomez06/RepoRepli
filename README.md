# RepoRepli
Home for the Reproducibility and Replicability Jupyter Notebook Questionnaire

Currently under construction. Please check back soon.

<center>
<img src="roadwork.jfif" alt="Roadwork Image" width="50%">
</center>



## Motivation

Replicability and Reproducibility can carry different meanings and connotations in different fields. In order to address the issue, we will use the terms as defined by the \[American\] Academies of Science. 

<big>Reproducibility</big> entails obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis.

<big>Replication</big> entails obtaining consistent results across studies, aimed at answering the same scientific question, each of which has obtained its own data. 

Reproducibility of results is taken as evidence of scientific robustness across numerous domains, and in the atmospheric sciences this can be evidenced by the higher-order requirements for benchmark databases and data replications efforts. However, the reproduction of scientific studies has been a focus of several works over concerns that these may be irreproducible or irreplicable. 

Efforts to address these concerns focus on scientists making their current work more reproducible, but we were unable to find guidance on what level of reproduction or replication of others’ work you should target, nor the types of claims you can make based on how successful you are in doing so.

This survey is split into two main portions:
>The first  portion evaluates how well you are able to replicate/reproduce the _fitting_ of the model in the base study.

>The second portion evaluates how well you are able to replicate/reproduce the _inference_ evaluation aspect of the study. 

## Instructions

In the questionnaire, you will be faced with a number of questions for which you will have to judge how well 



## References: <p style='text-indent: -1em; padding-left: 1em;'>
<sub> A. Ahadi, A. Hellas, P. Ihantola, A. Korhonen, and A. Petersen. Replication in computing education research: researcher attitudes and experiences. In Proceedings of the 16th Koli calling international conference on computing education research, pages 2–11, 2016.</sub> <p style='text-indent: -1em; padding-left: 1em;'>
<sub>M. Baker. 1,500 scientists lift the lid on reproducibility. Nature, 533(7604), 2016.</sub>
<p style='text-indent: -1em; padding-left: 1em;'> <sub>L. Cinquini, D. Crichton, C. Mattmann, J. Harney, G. Shipman, F. Wang, R. Ananthakrishnan, N. Miller, S. Denvil, M. Morgan, et al. The earth system grid federation: An open infrastructure for access to distributed geospatial data. Future Generation Computer Systems, 36:400–417, 2014.</sub>
<p style='text-indent: -1em; padding-left: 1em;'> <sub>B. Devezer, D. J. Navarro, J. Vandekerckhove, and E. Ozge Buzbas. The case for formal methodology in scientific reform. Royal Society open science, 8(3): 200805, 2021.</sub>
<p style='text-indent: -1em; padding-left: 1em;'> <sub>P. D. Dueben, M. G. Schultz, M. Chantry, D. J. Gagne, D. M. Hall, and A. McGovern. Challenges and benchmark datasets for machine learning in the atmospheric sciences: Definition, status, and outlook. Artificial Intelligence for the Earth Systems, 1(3):e210002, 2022.</sub>
<p style='text-indent: -1em; padding-left: 1em;'> <sub>W. M. Gervais. Practical methodological reform needs good theory. Perspectives on Psychological Science, 16(4):827–843, 2021.</sub>
<p style='text-indent: -1em; padding-left: 1em;'> <sub>S. Hoffmann, F. Sch ̈onbrodt, R. Elsas, R. Wilson, U. Strasser, and A.-L. Boulesteix. The multiplicity of analysis strategies jeopardizes replicability: lessons learned across disciplines. Royal Society Open Science, 8(4):201925, 2021</sub>
<p style='text-indent: -1em; padding-left: 1em;'> <sub>National Academies of Sciences, Engineering, and Medicine. 2019. Reproducibility and Replicability in Science. Washington, DC: The National Academies Press. https://doi.org/10.17226/25303.</sub> <p style='text-indent: -1em; padding-left: 1em;'>
<sub>J. D. Nichols, M. K. Oli, W. L. Kendall, and G. S. Boomer. A better approach for dealing with reproducibility and replicability in science. Proceedings of  the National Academy of Sciences, 118(7):e2100769118, 2021.</sub>
<p style='text-indent: -1em; padding-left: 1em;'> <sub>A. B. Schumacher, M. DeMaria, and J. A. Knaff. Objective estimation of the 24-h probability of tropical cyclone formation. Weather and Forecasting, 24 (2):456–471, 2009.</sub> <p style='text-indent: -1em; padding-left: 1em;'>
<sub>A. B. Schumacher, M. DeMaria, J. A. Knaff, and H. Syed. Updates to the NESDIS tropical cyclone formation probability product. 31st Conference on Hurricanes and Tropical Meteorology, 2014. URL https://ams.confex.com/ams/31Hurr/webprogram/Paper244172.html.</sub>